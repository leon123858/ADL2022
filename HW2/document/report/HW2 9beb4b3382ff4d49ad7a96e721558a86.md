# HW2

## Q1:**Data processing**

### Tokenizer

The tokenizer I used is bert-base-chinese.

It is an implementation for WordPiece.

Let me describe this algorithm briefly:

1. We should split all sentences into many minimal tokens
2. Learning the rules for merge token
    1. use the formula below to choose token pair
        
        `score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)`
        
    2. we can merge 2 tokens to get new token
    3. loop to get more merge rule until reach the desired vocabulary size
3. Use the rules to tokenize
    1. input sentence
    2. split to smallest tokens
    3. use best rule to merge tokens (Attempt to match tokens merged by the longest tokens)
4. There should be some special markup meaning, like split, start, end, …

### Answer Span

1. The dataset will give the target string and start position, so we can easily get the end position of the target string, after that, we should check all the tokens and get the result that belongs to the string. Finally, we get the tokenized location
2. We violently try all possible consecutive tokens, use the offset between the previously saved token and the actual content to get the corresponding substring, and finally select the substring with the highest score 

## **Q2: Modeling with BERTs and their variants**

### Describe

1. my model configure
    
    their are two NN model in this model
    
    first one is MC
    
    ```json
    {
      "_name_or_path": "ckiplab/albert-tiny-chinese",
      "architectures": [
        "AlbertForMultipleChoice"
      ],
      "attention_probs_dropout_prob": 0.0,
      "bos_token_id": 101,
      "classifier_dropout_prob": 0.1,
      "down_scale_factor": 1,
      "embedding_size": 128,
      "eos_token_id": 102,
      "gap_size": 0,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.0,
      "hidden_size": 312,
      "initializer_range": 0.02,
      "inner_group_num": 1,
      "intermediate_size": 1248,
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "albert",
      "net_structure_type": 0,
      "num_attention_heads": 12,
      "num_hidden_groups": 1,
      "num_hidden_layers": 4,
      "num_memory_blocks": 0,
      "pad_token_id": 0,
      "position_embedding_type": "absolute",
      "tokenizer_class": "BertTokenizerFast",
      "torch_dtype": "float32",
      "transformers_version": "4.23.1",
      "type_vocab_size": 2,
      "vocab_size": 21128
    }
    ```
    
    second one is QA
    
    ```json
    {
      "_name_or_path": "hfl/chinese-roberta-wwm-ext",
      "architectures": [
        "BertForQuestionAnswering"
      ],
      "attention_probs_dropout_prob": 0.1,
      "bos_token_id": 0,
      "classifier_dropout": null,
      "directionality": "bidi",
      "eos_token_id": 2,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.1,
      "hidden_size": 768,
      "initializer_range": 0.02,
      "intermediate_size": 3072,
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "bert",
      "num_attention_heads": 12,
      "num_hidden_layers": 12,
      "output_past": true,
      "pad_token_id": 0,
      "pooler_fc_size": 768,
      "pooler_num_attention_heads": 12,
      "pooler_num_fc_layers": 3,
      "pooler_size_per_head": 128,
      "pooler_type": "first_token_transform",
      "position_embedding_type": "absolute",
      "torch_dtype": "float32",
      "transformers_version": "4.23.1",
      "type_vocab_size": 2,
      "use_cache": true,
      "vocab_size": 21128
    }
    ```
    
2. performance of your model: **0.77667**
3. the loss function you used: 
    
    as source code
    
    ![Untitled](HW2%209beb4b3382ff4d49ad7a96e721558a86/Untitled.png)
    
    CrossEntropyLoss is the loss function
    
4. my optimizer: AdamW
    
    learning rate: 5e-5
    
    batch size: 16
    

### Try another type of pretrained model and describe

1. my model configure
    
    their are two NN model in this model
    
    because “ckiplab/albert-tiny-chinese” have small memory space and faster speed,
    
    so I just change QA NN to “ckiplab/albert-tiny-chinese”
    
    below is config
    
    ```json
    {
      "_name_or_path": "ckiplab/albert-tiny-chinese",
      "architectures": [
        "AlbertForQuestionAnswering"
      ],
      "attention_probs_dropout_prob": 0.0,
      "bos_token_id": 101,
      "classifier_dropout_prob": 0.1,
      "down_scale_factor": 1,
      "embedding_size": 128,
      "eos_token_id": 102,
      "gap_size": 0,
      "hidden_act": "gelu",
      "hidden_dropout_prob": 0.0,
      "hidden_size": 312,
      "initializer_range": 0.02,
      "inner_group_num": 1,
      "intermediate_size": 1248,
      "layer_norm_eps": 1e-12,
      "max_position_embeddings": 512,
      "model_type": "albert",
      "net_structure_type": 0,
      "num_attention_heads": 12,
      "num_hidden_groups": 1,
      "num_hidden_layers": 4,
      "num_memory_blocks": 0,
      "pad_token_id": 0,
      "position_embedding_type": "absolute",
      "tokenizer_class": "BertTokenizerFast",
      "torch_dtype": "float32",
      "transformers_version": "4.23.1",
      "type_vocab_size": 2,
      "vocab_size": 21128
    }
    ```
    
2. performance of your model: **0.53**
    
    I guess it's because the QA task is more complex, so the small model can't do it
    
3. the difference between pretrained model (architecture, pretraining loss, etc.)
    
    I will explain the difference between each model by talking about their pros and cons
    
    "ckiplab/albert-tiny-chinese": small memory space, fast training/prediction, but not very good for complex tasks
    
    "hfl/chinese-roberta-wwm-ext": large memory space, slow training/prediction, but good for complex tasks
    
4. I also tried using "hfl/chinese-roberta-wwm-ext" in the model per NN, but I found it was bigger and slower than my model, but didn't get a higher score than my model

## **Q3: Curves**

### loss function of QA

![圖片 1.png](HW2%209beb4b3382ff4d49ad7a96e721558a86/%25E5%259C%2596%25E7%2589%2587_1.png)

x-axis mean 500 batch (batch size = 16)

I draw it by excel, data come from `trainer_state.json` in out_dir for train script

### EM of QA

![圖片 1.png](HW2%209beb4b3382ff4d49ad7a96e721558a86/%25E5%259C%2596%25E7%2589%2587_1%201.png)

x-axis 1000 batch (batch size = 16)

I draw it by excel, data come from `eval_results.json` in out_dir for train script every 1000 batch (should set evaluate every 1000 batch)

## **Q4: Pretrained vs Not Pretrained**

I trained an unpretrained MC model

the config of it is below

```json
{
  "_name_or_path": "ckiplab/albert-tiny-chinese",
  "architectures": [
    "AlbertForMultipleChoice"
  ],
  "attention_probs_dropout_prob": 0.0,
  "bos_token_id": 101,
  "classifier_dropout_prob": 0.1,
  "down_scale_factor": 1,
  "embedding_size": 128,
  "eos_token_id": 102,
  "gap_size": 0,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "inner_group_num": 1,
  "intermediate_size": 1248,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "albert",
  "net_structure_type": 0,
  "num_attention_heads": 12,
  "num_hidden_groups": 1,
  "num_hidden_layers": 4,
  "num_memory_blocks": 0,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "tokenizer_class": "BertTokenizerFast",
  "torch_dtype": "float32",
  "transformers_version": "4.23.1",
  "type_vocab_size": 2,
  "vocab_size": 21128
}
```

I just cancel the pre-weight in origin model MC

the performance of this model is really bad

I draw the curve for it below

![圖片 1.png](HW2%209beb4b3382ff4d49ad7a96e721558a86/%25E5%259C%2596%25E7%2589%2587_1%202.png)

x-axis mean 500 batch (batch = 8)

We found that there is insufficient data to train such a model, and the loss is stable at 0.77 (trained model can be 0.2).

Therefore, the final accuracy is also small.

Here is the eval_metrics

![Untitled](HW2%209beb4b3382ff4d49ad7a96e721558a86/Untitled%201.png)

Also, I have tried some other models to solve this problem, but because this model has the fewest parameters, it has the best performance

### **Q5: Bonus: HW1 with BERTs**

you can check `/bonus` for more detail

1. your model
    1. intent: bert tiny
    2. slot: bert-base-uncased
2. performance of your model
    1. intent: 0.771
        
        ![Untitled](HW2%209beb4b3382ff4d49ad7a96e721558a86/Untitled%202.png)
        
    2. slot: 0.976
        
        ![Untitled](HW2%209beb4b3382ff4d49ad7a96e721558a86/Untitled%203.png)
        
3. the loss function you used
    1. intent
        
        by source code
        
        ![Untitled](HW2%209beb4b3382ff4d49ad7a96e721558a86/Untitled%204.png)
        
        CrossEntropyLoss is the loss function 
        
    2. slot
        
        as source code
        
        ![Untitled](HW2%209beb4b3382ff4d49ad7a96e721558a86/Untitled.png)
        
        CrossEntropyLoss is the loss function
        
4. The optimization algorithm (e.g. Adam), learning rate and batch size.
    1. intent
        - optimization algorithm: AdamW
        - learning rate: 3e-5
        - batch size: 16
    2. slot
        - optimization algorithm: AdamW
        - learning rate: 3e-5
        - batch size: 16